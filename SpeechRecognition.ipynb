{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b01b6f4-9007-45b3-b855-4a909821748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install speechbrain\n",
    "# !pip install ruamel_yaml\n",
    "# fsepteixeira \n",
    "# !pip install --upgrade ruamel.yaml --ignore-installed ruamel.yaml\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html \n",
    "# self.conv = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "# kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding),\n",
    "# PixelShuffle(upscale_factor),\n",
    "# nn.InstanceNorm2d(num_features=out_channels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8bedf4-d86c-4eee-ac18-ccc0b32f1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain as sb\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b6cd9f-3ecf-42e4-a48e-114e1320ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", \n",
    "                                       savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\",run_opts={\"device\":\"cuda\"},freeze_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3daa6ad5-7592-4656-8cc2-fd5358ef85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.hparams.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3973b9a9-d5e7-41a4-9f3c-fd5c30b9e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LIBRISPEECH(\".\", url= 'dev-clean',download=True)\n",
    "train_dataset = LIBRISPEECH(\".\",download=True)\n",
    "# dataset1 = MNIST(\".\",download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174612b5-a317-472b-a821-119ec178e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \"\"\"waveform, sample_rate, transcript, speaker_id\"\"\"\n",
    "    waveforms = [b[0].permute([1,0]) for b in batch]\n",
    "    waveforms = pad_sequence(waveforms,batch_first=True)\n",
    "    waveforms = waveforms.squeeze()\n",
    "    input_len = torch.FloatTensor([b[0].shape[1] for b in batch])\n",
    "    input_len /= torch.max(input_len)\n",
    "    sampling_rates = torch.FloatTensor([b[1] for b in batch])\n",
    "    transcript = [b[2] for b in batch]\n",
    "    speaker_id = torch.LongTensor([b[3] for b in batch])\n",
    "    \n",
    "    return waveforms,input_len,sampling_rates,transcript,speaker_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cc0cac-e7bc-4288-9ef9-37a2a10de96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataLoader = DataLoader(train_dataset,batch_size=24,shuffle=True,collate_fn=collate)\n",
    "val_dataLoader = DataLoader(val_dataset,batch_size=24,shuffle=True,collate_fn=collate)\n",
    "# train_dataloader = DataLoader(dataset1,batch_size=5,shuffle=True,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4e20c-59da-4295-ba1a-8c9c751a0247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1c097f-109c-40fe-93d1-867c0423d421",
   "metadata": {},
   "source": [
    "# highwat network\n",
    "- adopted from https://github.com/kefirski/pytorch_Highway.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17baf8f3-66f0-452e-aba0-b924f14c4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers, f):\n",
    "\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            :param x: tensor with shape of [batch_size, size]\n",
    "            :return: tensor with shape of [batch_size, size]\n",
    "            applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
    "            f is non-linear transformation, σ(x) is affine transformation with sigmoid non-linearition\n",
    "            and ⨀ is element-wise multiplication\n",
    "            \"\"\"\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "\n",
    "            nonlinear = self.f(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fb0e31-a7c9-4e3e-a02f-f909fa198229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecsReconstruction(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_features,num_blocks,hidden_size,out_feature=40):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.num_blocks = num_blocks\n",
    "        self.out_feature = out_feature\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_features,hidden_size) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.highway = Highway(hidden_size,num_blocks,nn.ReLU())\n",
    "        self.linear2 = nn.Linear(hidden_size,out_feature)\n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.highway(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6013450-9fdf-4d0c-9bdc-70bb263375f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = SpecsReconstruction(2560,5,100,40).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07308c23-e28d-4d49-b14d-3c07dd2af9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading states\n",
      "successifully loaded states\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.L1Loss()\n",
    "# optimizer = torch.optim.Adam(h0.parameters(),lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "# epochs = 30\n",
    "path = \"h1_states.pth\"\n",
    "def load_model_parameter(path):\n",
    "    try:\n",
    "        print(\"Loading states\")\n",
    "        state = torch.load(path)\n",
    "        start_epoch = state[\"epoch\"]\n",
    "        train_losses = state[\"train_losses\"]\n",
    "        val_losses = state[\"val_losses\"]\n",
    "        model_dict = state[\"model_dict\"]\n",
    "        optimizer = state[\"optimizer\"]\n",
    "        scheduler = state[\"scheduler\"]\n",
    "        print(\"successifully loaded states\")\n",
    "        return start_epoch,train_losses,val_losses,model_dict,optimizer,scheduler\n",
    "    except:\n",
    "        print(\"failed to load states\")\n",
    "        return None\n",
    "\n",
    "def save_model_parameters(path,state_dict):\n",
    "    torch.save(state_dict,path)\n",
    "    print(\"states at {} epoch saved\".format(state_dict[\"epoch\"]))\n",
    "    \n",
    "states = load_model_parameter(path)\n",
    "if states is not None:\n",
    "    start_epoch,train_losses,val_losses,model_dict,optimizer_dict,scheduler_dict = states\n",
    "    h1.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "    scheduler.load_state_dict(scheduler_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9778e-bc64-447d-b202-08a43efcc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h0.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "        block_0 = block_0.reshape(block_0.shape[0],block_0.shape[1],-1)\n",
    "        out = h0(block_0)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h0.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            block_0 = block_0.reshape(block_0.shape[0],block_0.shape[1],-1)\n",
    "            out = h0(block_0)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h0,\"best_model_0\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h0.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h0_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71068177-49a6-4987-85ce-43779fa56c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del h0\n",
    "h1 = SpecsReconstruction(2560,5,100,40).cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(h1.parameters(),lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f34396-de68-4106-8f99-0acd6fa4541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/30\n",
      "epoch:1/30\n",
      "epoch:1/30\n",
      "epoch:1/30\n",
      "epoch:1/30\n",
      "epoch:1/30\n",
      "epoch:1/30\n",
      "epoch:1/30,Train loss:0.1295041365887198\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.99 GiB (GPU 0; 14.76 GiB total capacity; 6.71 GiB already allocated; 1.98 GiB free; 11.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b93ef86ed62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mblock_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mblock_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mblock_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/speechbrain/nnet/containers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/speechbrain/nnet/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0minput\u001b[0m \u001b[0mto\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0md\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0mare\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         )\n\u001b[0;32m-> 2347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.99 GiB (GPU 0; 14.76 GiB total capacity; 6.71 GiB already allocated; 1.98 GiB free; 11.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h1.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            block_1 = encoder.model.CNN.block_1( block_0)\n",
    "            block_1 = block_1.reshape(block_1.shape[0],block_1.shape[1],-1)\n",
    "        out = h1(block_1)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:inputs\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h1.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            block_1 = encoder.model.CNN.block_1(block_0)\n",
    "            block_1 = block_1.reshape(block_1.shape[0],block_1.shape[1],-1)\n",
    "            out = h1(block_1)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h1,\"best_model_1\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h1.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h1_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f158d2-2bec-4ad6-9531-97502f14d127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading states\n",
      "successifully loaded states\n"
     ]
    }
   ],
   "source": [
    "h1_1 = SpecsReconstruction(5120,5,100,40).cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(h1_1.parameters(),lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "epochs = 30\n",
    "stretch = torchaudio.transforms.TimeStretch()\n",
    "\n",
    "path = \"h1_1_states.pth\"\n",
    "def load_model_parameter(path):\n",
    "    try:\n",
    "        print(\"Loading states\")\n",
    "        state = torch.load(path)\n",
    "        start_epoch = state[\"epoch\"]\n",
    "        train_losses = state[\"train_losses\"]\n",
    "        val_losses = state[\"val_losses\"]\n",
    "        model_dict = state[\"model_dict\"]\n",
    "        optimizer = state[\"optimizer\"]\n",
    "        scheduler = state[\"scheduler\"]\n",
    "        print(\"successifully loaded states\")\n",
    "        return start_epoch,train_losses,val_losses,model_dict,optimizer,scheduler\n",
    "    except:\n",
    "        print(\"failed to load states\")\n",
    "        return None\n",
    "\n",
    "def save_model_parameters(path,state_dict):\n",
    "    torch.save(state_dict,path)\n",
    "    print(\"states at {} epoch saved\".format(state_dict[\"epoch\"]))\n",
    "    \n",
    "states = load_model_parameter(path)\n",
    "if states is not None:\n",
    "    start_epoch,train_losses,val_losses,model_dict,optimizer_dict,scheduler_dict = states\n",
    "    h1_1.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "    scheduler.load_state_dict(scheduler_dict)\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6edcdc9-7289-473b-bd1e-18f8dbfbc055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30\n",
      "epoch:4/30,Train loss:0.07184761008482521\n",
      "epoch:4/30\n",
      "epoch:4/30,Val loss:0.04556572872453031\n",
      "states at 3 epoch saved\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30\n",
      "epoch:5/30,Train loss:0.0682624492262091\n",
      "epoch:5/30\n",
      "epoch:5/30,Val loss:0.054650104151362865\n",
      "states at 4 epoch saved\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30\n",
      "epoch:6/30,Train loss:0.06298657956869662\n",
      "epoch:6/30\n",
      "epoch:6/30,Val loss:0.03953117112406587\n",
      "states at 5 epoch saved\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30\n",
      "epoch:7/30,Train loss:0.06011461657198036\n",
      "epoch:7/30\n",
      "epoch:7/30,Val loss:0.04646785352873591\n",
      "states at 6 epoch saved\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30\n",
      "epoch:8/30,Train loss:0.058711602635869456\n",
      "epoch:8/30\n",
      "epoch:8/30,Val loss:0.039200594681686005\n",
      "states at 7 epoch saved\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30\n",
      "epoch:9/30,Train loss:0.05648555431921943\n",
      "epoch:9/30\n",
      "epoch:9/30,Val loss:0.03552716418243615\n",
      "states at 8 epoch saved\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30\n",
      "epoch:10/30,Train loss:0.05524202102184796\n",
      "epoch:10/30\n",
      "epoch:10/30,Val loss:0.031830389003178715\n",
      "states at 9 epoch saved\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30\n",
      "epoch:11/30,Train loss:0.05355539305307785\n",
      "epoch:11/30\n",
      "epoch:11/30,Val loss:0.03334931000671555\n",
      "states at 10 epoch saved\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30\n",
      "epoch:12/30,Train loss:0.05226711433913026\n",
      "epoch:12/30\n",
      "epoch:12/30,Val loss:0.03742546588182449\n",
      "states at 11 epoch saved\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30\n",
      "epoch:13/30,Train loss:0.050538543956119474\n",
      "epoch:13/30\n",
      "epoch:13/30,Val loss:0.03990267919360009\n",
      "states at 12 epoch saved\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30\n",
      "epoch:14/30,Train loss:0.040523511202645905\n",
      "epoch:14/30\n",
      "epoch:14/30,Val loss:0.022366848403373652\n",
      "states at 13 epoch saved\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30\n",
      "epoch:15/30,Train loss:0.03992977989759265\n",
      "epoch:15/30\n",
      "epoch:15/30,Val loss:0.020893293111461455\n",
      "states at 14 epoch saved\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30\n",
      "epoch:16/30,Train loss:0.039487449640110764\n",
      "epoch:16/30\n",
      "epoch:16/30,Val loss:0.021298164444093683\n",
      "states at 15 epoch saved\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30\n",
      "epoch:17/30,Train loss:0.0392369747741007\n",
      "epoch:17/30\n",
      "epoch:17/30,Val loss:0.020775772441607134\n",
      "states at 16 epoch saved\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30\n",
      "epoch:18/30,Train loss:0.03885539801900878\n",
      "epoch:18/30\n",
      "epoch:18/30,Val loss:0.02576369722991918\n",
      "states at 17 epoch saved\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30\n",
      "epoch:19/30,Train loss:0.03846190181412116\n",
      "epoch:19/30\n",
      "epoch:19/30,Val loss:0.020261573547546843\n",
      "states at 18 epoch saved\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30\n",
      "epoch:20/30,Train loss:0.03791548424721265\n",
      "epoch:20/30\n",
      "epoch:20/30,Val loss:0.021701395602286917\n",
      "states at 19 epoch saved\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30\n",
      "epoch:21/30,Train loss:0.0373895815910161\n",
      "epoch:21/30\n",
      "epoch:21/30,Val loss:0.019451570960866138\n",
      "states at 20 epoch saved\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30\n",
      "epoch:22/30,Train loss:0.03703975456477213\n",
      "epoch:22/30\n",
      "epoch:22/30,Val loss:0.02018056531329598\n",
      "states at 21 epoch saved\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30\n",
      "epoch:23/30,Train loss:0.03675053825429758\n",
      "epoch:23/30\n",
      "epoch:23/30,Val loss:0.020027190133665516\n",
      "states at 22 epoch saved\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30\n",
      "epoch:24/30,Train loss:0.0365827131484236\n",
      "epoch:24/30\n",
      "epoch:24/30,Val loss:0.020934904241456394\n",
      "states at 23 epoch saved\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30\n",
      "epoch:25/30,Train loss:0.03570358291925753\n",
      "epoch:25/30\n",
      "epoch:25/30,Val loss:0.018524225702327965\n",
      "states at 24 epoch saved\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30\n",
      "epoch:26/30,Train loss:0.03567042382856628\n",
      "epoch:26/30\n",
      "epoch:26/30,Val loss:0.018768197939261398\n",
      "states at 25 epoch saved\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30\n",
      "epoch:27/30,Train loss:0.03562931630666517\n",
      "epoch:27/30\n",
      "epoch:27/30,Val loss:0.018558938418869422\n",
      "states at 26 epoch saved\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30\n",
      "epoch:28/30,Train loss:0.03553374098973865\n",
      "epoch:28/30\n",
      "epoch:28/30,Val loss:0.01849736785104053\n",
      "states at 27 epoch saved\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30\n",
      "epoch:29/30,Train loss:0.03550535453436505\n",
      "epoch:29/30\n",
      "epoch:29/30,Val loss:0.018666152084097925\n",
      "states at 28 epoch saved\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30\n",
      "epoch:30/30,Train loss:0.03547174070938295\n",
      "epoch:30/30\n",
      "epoch:30/30,Val loss:0.018072876700125966\n",
      "states at 29 epoch saved\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(start_epoch,epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h1_1.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            conv_1 = encoder.model.CNN.block_1.conv_1(block_0)\n",
    "            norm_1 = encoder.model.CNN.block_1.norm_1(conv_1)\n",
    "            norm_1 =  norm_1.reshape( norm_1.shape[0], norm_1.shape[1],-1)\n",
    "        out = h1_1(norm_1)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h1_1.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            conv_1 = encoder.model.CNN.block_1.conv_1(block_0)\n",
    "            norm_1 = encoder.model.CNN.block_1.norm_1(conv_1)\n",
    "            norm_1 =  norm_1.reshape( norm_1.shape[0], norm_1.shape[1],-1)\n",
    "            out = h1_1(norm_1)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h1_1,\"best_model_11\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h1_1.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h1_1_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80089d2-5920-4540-b5fa-9a5d8e32753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4672b-a6e7-4e6e-9cfb-fafdaea8aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch = torchaudio.transforms.TimeStretch()\n",
    "specs = torchaudio.transforms.Spectrogram()\n",
    "a = specs(torch.randn((40,100)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb446c0f-53bb-4cf0-bfa1-b12797dbe4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = specs(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c716b-a23c-4981-9159-40683d16cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch(a,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da06983-d6e9-4e49-830d-6fb58c155a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
